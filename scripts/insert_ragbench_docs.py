#!/usr/bin/env python3
"""
ä¸ºRAGBenchç›¸å…³çš„ç©ºçŸ¥è¯†åº“æ’å…¥æ–‡æ¡£

æ­¤è„šæœ¬ä¼šï¼š
1. è·å–æ‰€æœ‰çŸ¥è¯†åº“åˆ—è¡¨
2. è¯†åˆ«RAGBenchç›¸å…³çš„ç©ºçŸ¥è¯†åº“
3. ä»parquetæ–‡ä»¶æå–æ–‡æ¡£å†…å®¹
4. åˆ›å»ºtxtæ–‡ä»¶å¹¶ä¸Šä¼ åˆ°å¯¹åº”çŸ¥è¯†åº“
5. ä½¿ç”¨ä¼˜åŒ–çš„æ¨¡å‹é…ç½®å’Œå¤„ç†è§„åˆ™
"""

import os
import json
import requests
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Set
import time
from tqdm import tqdm

# å¯¼å…¥é…ç½®
try:
    from insert_config import (
        DIFY_CONFIG, RAGBENCH_CONFIG, DATASET_CONFIGS, 
        DEFAULT_MODELS, PROCESS_RULES, UPLOAD_CONFIG
    )
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    # é»˜è®¤é…ç½®
    DIFY_CONFIG = {
        "base_url": "http://192.168.31.205",
        "api_key": "dataset-L7pHf6iaAwImkw5601pv3N2u"
    }
    RAGBENCH_CONFIG = {
        "data_path": "data/ragbench",
        "max_documents_per_dataset": 50,
        "upload_delay": 0.2,
        "temp_dir": "temp_docs"
    }
    DEFAULT_MODELS = {
        "embedding_model": {"provider": "siliconflow", "model": "BAAI/bge-m3"},
        "retrieval_model": {"search_method": "hybrid_search", "reranking_enable": True}
    }
    PROCESS_RULES = {"mode": "custom"}

class RAGBenchDocumentInserter:
    def __init__(self, dify_base_url: str, api_key: str):
        """
        åˆå§‹åŒ–RAGBenchæ–‡æ¡£æ’å…¥å™¨
        
        Args:
            dify_base_url: DifyæœåŠ¡å™¨åœ°å€
            api_key: Dify APIå¯†é’¥
        """
        self.base_url = dify_base_url.rstrip('/')
        self.api_key = api_key
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„RAGBenchæ•°æ®é›†é…ç½®
        self.ragbench_datasets = DATASET_CONFIGS
        
        # æ¨¡å‹é…ç½®
        self.model_config = DEFAULT_MODELS
        self.process_rules = PROCESS_RULES
        
        print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹é…ç½®:")
        print(f"   åµŒå…¥æ¨¡å‹: {self.model_config['embedding_model']['model']}")
        print(f"   æ£€ç´¢æ–¹æ³•: {self.model_config['retrieval_model']['search_method']}")
        print(f"   é‡æ’åº: {'å¯ç”¨' if self.model_config['retrieval_model']['reranking_enable'] else 'ç¦ç”¨'}")
    
    def get_all_knowledge_bases(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰çŸ¥è¯†åº“åˆ—è¡¨
        
        Returns:
            çŸ¥è¯†åº“åˆ—è¡¨
        """
        url = f"{self.base_url}/v1/datasets?include_all=true"
        
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            
            result = response.json()
            knowledge_bases = result.get("data", [])
            
            print(f"âœ… æˆåŠŸè·å–çŸ¥è¯†åº“åˆ—è¡¨")
            print(f"   æ€»æ•°: {len(knowledge_bases)}")
            
            return knowledge_bases
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def identify_ragbench_kbs(self, knowledge_bases: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        è¯†åˆ«RAGBenchç›¸å…³çš„çŸ¥è¯†åº“
        
        Args:
            knowledge_bases: çŸ¥è¯†åº“åˆ—è¡¨
            
        Returns:
            RAGBenchçŸ¥è¯†åº“å­—å…¸ {name: kb_info}
        """
        ragbench_kbs = {}
        
        for kb in knowledge_bases:
            name = kb.get('name', '')
            doc_count = kb.get('document_count', 0)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯RAGBenchæ•°æ®é›†
            if name in self.ragbench_datasets:
                priority = self.ragbench_datasets[name].get('priority', 1)
                ragbench_kbs[name] = {
                    'kb_info': kb,
                    'has_documents': doc_count > 0,
                    'document_count': doc_count,
                    'priority': priority,
                    'description': self.ragbench_datasets[name].get('description', '')
                }
                print(f"ğŸ“š å‘ç°RAGBenchçŸ¥è¯†åº“: {name} (æ–‡æ¡£æ•°: {doc_count}, ä¼˜å…ˆçº§: {priority})")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_kbs = dict(sorted(ragbench_kbs.items(), key=lambda x: x[1]['priority']))
        return sorted_kbs
    
    def extract_documents_from_parquet(self, dataset_name: str, ragbench_path: str, max_docs: int = 50) -> List[str]:
        """
        ä»parquetæ–‡ä»¶æå–æ–‡æ¡£å†…å®¹
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            ragbench_path: RAGBenchæ•°æ®è·¯å¾„
            max_docs: æœ€å¤§æ–‡æ¡£æ•°é‡
            
        Returns:
            æ–‡æ¡£å†…å®¹åˆ—è¡¨
        """
        dataset_path = Path(ragbench_path) / dataset_name
        train_file = dataset_path / "train-00000-of-00001.parquet"
        
        if not train_file.exists():
            print(f"  âš ï¸  è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
            return []
        
        try:
            # è¯»å–parquetæ–‡ä»¶
            df = pd.read_parquet(train_file)
            print(f"  ğŸ“– è¯»å–åˆ° {len(df)} æ¡è®°å½•")
            
            # æå–æ–‡æ¡£å†…å®¹
            documents = []
            if 'documents' in df.columns:
                # å¦‚æœdocumentsåˆ—æ˜¯åˆ—è¡¨ï¼Œå±•å¼€å®ƒ
                for idx, row in df.iterrows():
                    docs = row['documents']
                    if isinstance(docs, list):
                        documents.extend(docs)
                    else:
                        documents.append(str(docs))
            elif 'question' in df.columns and 'response' in df.columns:
                # å¦‚æœæ²¡æœ‰documentsåˆ—ï¼Œä½¿ç”¨questionå’Œresponseç»„åˆ
                for idx, row in df.iterrows():
                    content = f"Question: {row['question']}\n\nAnswer: {row['response']}"
                    documents.append(content)
            elif 'context' in df.columns and 'question' in df.columns:
                # å¦‚æœæœ‰contextå’Œquestionåˆ—
                for idx, row in df.iterrows():
                    content = f"Context: {row['context']}\n\nQuestion: {row['question']}"
                    if 'answer' in df.columns:
                        content += f"\n\nAnswer: {row['answer']}"
                    documents.append(content)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_docs = list(set(documents))
            selected_docs = unique_docs[:max_docs]
            
            print(f"  ğŸ“„ æå–åˆ° {len(selected_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")
            return selected_docs
            
        except Exception as e:
            print(f"  âŒ å¤„ç†æ•°æ®é›†å¤±è´¥ {dataset_name}: {e}")
            return []
    
    def create_txt_file(self, content: str, filename: str, output_dir: str = None) -> str:
        """
        åˆ›å»ºtxtæ–‡ä»¶
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æ–‡ä»¶è·¯å¾„
        """
        if output_dir is None:
            output_dir = RAGBENCH_CONFIG.get("temp_dir", "temp_docs")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ¸…ç†æ–‡ä»¶å
        safe_filename = "".join(c for c in filename if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_filename = safe_filename.replace(' ', '_')
        
        file_path = os.path.join(output_dir, f"{safe_filename}.txt")
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return file_path
        except Exception as e:
            print(f"  âŒ åˆ›å»ºæ–‡ä»¶å¤±è´¥ {filename}: {e}")
            return None
    
    def upload_document_to_kb(self, kb_id: str, file_path: str, filename: str) -> bool:
        """
        ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
            
        Returns:
            æ˜¯å¦ä¸Šä¼ æˆåŠŸ
        """
        url = f"{self.base_url}/v1/datasets/{kb_id}/document/create-by-file"
        
        try:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¤„ç†è§„åˆ™
            process_data = {
                "indexing_technique": UPLOAD_CONFIG.get("indexing_technique", "high_quality"),
                "process_rule": self.process_rules
            }
            
            # ä¸Šä¼ æ–‡ä»¶
            with open(file_path, 'rb') as f:
                files = {'file': (f"{filename}.txt", f, 'text/plain')}
                data = {
                    'data': (None, json.dumps(process_data), 'application/json')
                }
                
                response = requests.post(url, files=files, data=data, headers={
                    "Authorization": f"Bearer {self.api_key}"
                })
                
                response.raise_for_status()
                print(f"  âœ… ä¸Šä¼ æˆåŠŸ: {filename}")
                return True
                
        except Exception as e:
            print(f"  âŒ ä¸Šä¼ å¤±è´¥ {filename}: {e}")
            return False
    
    def process_ragbench_dataset(self, dataset_name: str, kb_info: Dict[str, Any], 
                                ragbench_path: str, max_docs: int = 50) -> None:
        """
        å¤„ç†å•ä¸ªRAGBenchæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            kb_info: çŸ¥è¯†åº“ä¿¡æ¯
            ragbench_path: RAGBenchæ•°æ®è·¯å¾„
            max_docs: æœ€å¤§æ–‡æ¡£æ•°é‡
        """
        print(f"\nğŸ“š å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"   æè¿°: {kb_info.get('description', 'N/A')}")
        print(f"   ä¼˜å…ˆçº§: {kb_info.get('priority', 'N/A')}")
        
        kb_id = kb_info['kb_info']['id']
        doc_count = kb_info['document_count']
        
        if doc_count > 0:
            print(f"  âš ï¸  çŸ¥è¯†åº“å·²æœ‰ {doc_count} ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡")
            return
        
        print(f"  ğŸ†• çŸ¥è¯†åº“ä¸ºç©ºï¼Œå¼€å§‹æ’å…¥æ–‡æ¡£...")
        
        # æå–æ–‡æ¡£å†…å®¹
        documents = self.extract_documents_from_parquet(dataset_name, ragbench_path, max_docs)
        
        if not documents:
            print(f"  âŒ æ— æ³•æå–æ–‡æ¡£å†…å®¹")
            return
        
        # ä¸Šä¼ æ–‡æ¡£
        success_count = 0
        for i, doc_content in enumerate(tqdm(documents, desc=f"ä¸Šä¼  {dataset_name}")):
            if doc_content and str(doc_content).strip():
                filename = f"{dataset_name}_doc_{i+1:04d}"
                
                # åˆ›å»ºtxtæ–‡ä»¶
                file_path = self.create_txt_file(doc_content, filename)
                if file_path:
                    # ä¸Šä¼ åˆ°çŸ¥è¯†åº“
                    if self.upload_document_to_kb(kb_id, file_path, filename):
                        success_count += 1
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(file_path)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    delay = RAGBENCH_CONFIG.get("upload_delay", 0.2)
                    time.sleep(delay)
        
        print(f"  âœ… æˆåŠŸä¸Šä¼  {success_count}/{len(documents)} ä¸ªæ–‡æ¡£")
    
    def insert_all_ragbench_docs(self, ragbench_path: str, max_docs_per_dataset: int = None) -> None:
        """
        ä¸ºæ‰€æœ‰RAGBenchçŸ¥è¯†åº“æ’å…¥æ–‡æ¡£
        
        Args:
            ragbench_path: RAGBenchæ•°æ®è·¯å¾„
            max_docs_per_dataset: æ¯ä¸ªæ•°æ®é›†æœ€å¤§æ–‡æ¡£æ•°é‡
        """
        if max_docs_per_dataset is None:
            max_docs_per_dataset = RAGBENCH_CONFIG.get("max_documents_per_dataset", 50)
        
        print(f"ğŸš€ å¼€å§‹ä¸ºRAGBenchçŸ¥è¯†åº“æ’å…¥æ–‡æ¡£")
        print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {ragbench_path}")
        print(f"ğŸ”‘ DifyæœåŠ¡å™¨: {self.base_url}")
        print(f"ğŸ“Š æ¯ä¸ªæ•°æ®é›†æœ€å¤§æ–‡æ¡£æ•°: {max_docs_per_dataset}")
        print(f"â±ï¸  ä¸Šä¼ å»¶è¿Ÿ: {RAGBENCH_CONFIG.get('upload_delay', 0.2)}ç§’")
        print("=" * 60)
        
        # è·å–æ‰€æœ‰çŸ¥è¯†åº“
        knowledge_bases = self.get_all_knowledge_bases()
        if not knowledge_bases:
            print("âŒ æ— æ³•è·å–çŸ¥è¯†åº“åˆ—è¡¨")
            return
        
        # è¯†åˆ«RAGBenchçŸ¥è¯†åº“
        ragbench_kbs = self.identify_ragbench_kbs(knowledge_bases)
        
        if not ragbench_kbs:
            print("âŒ æœªæ‰¾åˆ°RAGBenchç›¸å…³çš„çŸ¥è¯†åº“")
            return
        
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(ragbench_kbs)} ä¸ªRAGBenchçŸ¥è¯†åº“")
        
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†æ¯ä¸ªæ•°æ®é›†
        for dataset_name, kb_info in ragbench_kbs.items():
            self.process_ragbench_dataset(dataset_name, kb_info, ragbench_path, max_docs_per_dataset)
        
        print(f"\nğŸ‰ æ‰€æœ‰RAGBenchæ•°æ®é›†å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        for dataset_name, kb_info in ragbench_kbs.items():
            status = "âœ… æœ‰æ–‡æ¡£" if kb_info['has_documents'] else "ğŸ†• å·²æ’å…¥æ–‡æ¡£"
            priority = kb_info.get('priority', 'N/A')
            print(f"  {dataset_name}: {status} (ä¼˜å…ˆçº§: {priority})")


def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é…ç½®
    dify_base_url = DIFY_CONFIG.get("base_url", "http://192.168.31.205")
    dify_api_key = DIFY_CONFIG.get("api_key", "dataset-L7pHf6iaAwImkw5601pv3N2u")
    ragbench_path = RAGBENCH_CONFIG.get("data_path", "data/ragbench")
    max_docs = RAGBENCH_CONFIG.get("max_documents_per_dataset", 50)
    
    print("ğŸš€ RAGBenchæ–‡æ¡£æ’å…¥å·¥å…·")
    print("=" * 50)
    print(f"DifyæœåŠ¡å™¨: {dify_base_url}")
    print(f"RAGBenchè·¯å¾„: {ragbench_path}")
    print(f"æœ€å¤§æ–‡æ¡£æ•°/æ•°æ®é›†: {max_docs}")
    print()
    
    # æ£€æŸ¥RAGBenchè·¯å¾„
    if not Path(ragbench_path).exists():
        print(f"âŒ RAGBenchè·¯å¾„ä¸å­˜åœ¨: {ragbench_path}")
        return
    
    # åˆ›å»ºæ’å…¥å™¨å¹¶æ‰§è¡Œ
    inserter = RAGBenchDocumentInserter(dify_base_url, dify_api_key)
    inserter.insert_all_ragbench_docs(ragbench_path, max_docs)


if __name__ == "__main__":
    main()
